\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{bm}
\usepackage{geometry}
\usepackage{mathtools}
\geometry{margin=1in}

\title{Learning the Enhanced ToM Observer: Training Objectives and Loops}
\author{}
\date{}

\begin{document}
\maketitle

\section{Problem Setup}

The observer is trained in supervised fashion to predict the next action of an agent in a gridworld from three inputs: a character embedding $c\in\mathbb{R}^{d_c}$, a mental-window embedding $m\in\mathbb{R}^{d_m}$, and a current state vector $x\in\mathbb{R}^{d_x}$. Each training example consists of
\[
  (c_i, m_i, x_i, y_i, s_i),\qquad y_i\in\{0,1,2,3,4\},\; s_i\in\{0,1\},
\]
where $y_i$ is the discrete action label (Up/Down/Left/Right/Stay) and $s_i$ is the dataset-provided indicator of a ``false-belief'' scenario (recent swap outside field-of-view). Batches are sampled via a \texttt{DataLoader}.

Intuitively, the model learns a mapping from multi-source context $(c,m,x)$ to the agent's imminent action. The character vector summarizes longer-horizon tendencies; the mental window contains short-horizon contextual cues; the state vector encodes the immediate world configuration. Supervision uses action choices generated by agents (rule-based or learned) in simulated rollouts, allowing us to compare architectures under identical data.

\paragraph{Full-state vector $x$ (17-D).} By default, $x$ is a compact encoding of the full environment state relative to the agent:
\begin{itemize}
  \item \textbf{Agent position} $(x,y)$ normalized to $[-1,1]$ (2)
  \item \textbf{Subgoal offset} relative to the agent $(\Delta x,\Delta y)$ normalized by grid size (2)
  \item \textbf{Four objects} offsets: for each object $k\in\{0,1,2,3\}$, $(\Delta x_k,\Delta y_k)$ relative to the agent (8 total)
  \item \textbf{Last action} one-hot over $\{\textsc{Up},\textsc{Down},\textsc{Left},\textsc{Right},\textsc{Stay}\}$ (5)
\end{itemize}
Total: $2+2+8+5=17$.

\section{Data and Batching Pipeline}

Training data are produced by simulated rollouts and wrapped in a dataset class that yields tensors $(c,m,x,y,s)$ per step. A data loader shuffles training samples and forms mini-batches of size $B$ for stochastic optimization. This achieves two goals: (i) efficient vectorized computation on accelerators and (ii) noisy gradient estimates that often generalize better than full-batch gradients.

At a high level, one epoch consists of iterating over all mini-batches once (order reshuffled each epoch), computing the loss and its gradient on each batch, and stepping the optimizer. Validation follows the same batching pattern but disables gradient computation for speed and correctness.

\section{Model and Outputs}

Let $f_{\bm{\theta}}$ denote the observer (classical, quantum, or hybrid), with parameters $\bm{\theta}$ collecting all classical weights and any quantum variational parameters. The model outputs a vector of logits $\bm{z}_i \in \mathbb{R}^5$ and induced class probabilities via the softmax
\begin{equation}
  \bm{z}_i = f_{\bm{\theta}}(c_i, m_i, x_i),\qquad
  p_i(k) = \mathrm{softmax}(\bm{z}_i)_k = \frac{e^{z_{i,k}}}{\sum_{j=1}^{5} e^{z_{i,j}}}.
\end{equation}
Predicted labels are $\hat{y}_i = \arg\max_k p_i(k)$ during evaluation.

The 5 output logits correspond to the discrete action set: Up, Down, Left, Right, and Stay. Using logits rather than probabilities is numerically stable and integrates cleanly with the cross-entropy loss used by deep learning libraries. Internally, the observer fuses $(c,m)$ with either a classical, quantum, or hybrid representation derived from $x$ (state encoder) and, optionally, a belief encoder; however, the training objective and optimizer are identical across these variants.

\section{Training Objective}

The primary loss is the cross-entropy between the predicted distribution and the true action label. For a batch $\mathcal{B}$ of size $B$,
\begin{equation}
  \mathcal{L}(\bm{\theta}; \mathcal{B}) = -\frac{1}{B} \sum_{(c,m,x,y)\in\mathcal{B}} \log p_{\bm{\theta}}\big(y\,\big|\,c,m,x\big)
  = \frac{1}{B} \sum_{(c,m,x,y)\in\mathcal{B}} \ell\big(f_{\bm{\theta}}(c,m,x), y\big),
\end{equation}
where $\ell(\bm{z}, y)$ is the standard softmax cross-entropy. Dropout in some classical layers provides implicit regularization; no explicit weight decay is required in default runs.

Cross-entropy encourages the logit of the correct action to be larger than the others, shaping the decision boundaries in feature space. Because the labels are one of five classes, this objective naturally handles multi-class settings. If desired, the framework can incorporate class weights (for imbalanced data) or auxiliary regularizers; in our experiments, the basic formulation suffices.

\section{Optimization}

Parameters are updated with Adam at learning rate $\eta$ (default $\eta=3\times10^{-4}$). Writing a generic parameter as $\theta$, Adam maintains first and second moment estimates $(m_t, v_t)$ and applies
\begin{align}
  m_t &= \beta_1 m_{t-1} + (1-\beta_1) \, g_t, & \hat{m}_t &= \frac{m_t}{1-\beta_1^t},\\
  v_t &= \beta_2 v_{t-1} + (1-\beta_2) \, g_t^2, & \hat{v}_t &= \frac{v_t}{1-\beta_2^t},\\
  \theta_{t+1} &= \theta_t - \eta \, \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\epsilon},
\end{align}
with gradient $g_t = \partial \mathcal{L}/\partial \theta$. All classical and quantum parameters participate equally in this update.

Adam adapts the learning rate per parameter based on the history of gradients, which typically accelerates convergence and improves robustness to hyperparameter choices. In practice, a modest batch size (e.g., 64--128) and the default Adam coefficients $(\beta_1,\beta_2,\epsilon)=(0.9,0.999,10^{-8})$ work well. Gradient clipping can be added when training very deep classical heads, though it was not necessary in our setting.

\section{Quantum Gradients and Autodiff}

When the model contains quantum layers (state or belief encoders), their outputs are differentiable w.r.t. circuit parameters through the \emph{parameter-shift rule}. If an expectation feature has form $\langle M \rangle(\theta)$ for a gate $e^{-i\theta H/2}$ with $H^2=I$, then
\begin{equation}
  \frac{\partial \, \langle M \rangle}{\partial \theta}
  = \tfrac{1}{2} \Big( \langle M \rangle\big(\theta + \tfrac{\pi}{2}\big) - \langle M \rangle\big(\theta - \tfrac{\pi}{2}\big) \Big).
\end{equation}
In practice, the quantum circuit is exposed as a \texttt{TorchLayer} with interface ``torch'', so standard backpropagation seamlessly propagates gradients through the circuit into both the variational angles and the preceding input projection layer.

Conceptually, the classical network sends a real-valued vector to the quantum layer (after a learned linear projection), the circuit produces expectation values (real features), and the classical post-processing maps these to the model’s embedding size. Because the entire pipeline is differentiable, errors computed on the 5-way action prediction flow backward to update both classical weights and quantum rotation angles, aligning the quantum features with the task.

\section{Epoch Loop and Batching}

Let $\{\mathcal{B}_t\}_{t=1}^{T}$ be the batches in one epoch. The training epoch performs for each batch:
\begin{enumerate}
  \item Move tensors $(c,m,x,y)$ to the device (CPU/GPU).
  \item Forward pass to compute logits $\bm{z}$ and loss $\mathcal{L}$ on the batch.
  \item Zero optimizer gradients, backpropagate $\nabla_{\bm{\theta}} \mathcal{L}$, and apply an optimizer step (Adam).
  \item Accumulate the batch loss scaled by batch size to compute epoch-average loss.
\end{enumerate}
Formally, the epoch objective approximates the empirical risk $\tfrac{1}{N}\sum_{i=1}^N \ell(f_{\bm{\theta}}(c_i,m_i,x_i), y_i)$ via mini-batch stochastic gradients.

For reference, a concise pseudocode of the training loop is:
\begin{verbatim}
for epoch in 1..E:
    model.train()
    for (c, m, x, y, _) in train_loader:
        opt.zero_grad()
        z = model(c, m, x)          # forward
        L = cross_entropy(z, y)
        L.backward()                 # gradients (includes quantum via parameter-shift)
        opt.step()                   # Adam update
    model.eval(); evaluate(val_loader)  # no grad, report metrics
\end{verbatim}

\section{Evaluation Metrics}

After each epoch, the model is evaluated on a held-out dataset without gradient updates. We report:
\begin{itemize}
  \item \textbf{Average loss}: $\overline{\mathcal{L}}=\tfrac{1}{N}\sum_i \ell(\bm{z}_i, y_i)$.
  \item \textbf{Overall accuracy}: $\mathrm{Acc}=\tfrac{1}{N}\sum_i \mathbf{1}\{\hat{y}_i=y_i\}$.
  \item \textbf{False-belief accuracy}: $\mathrm{Acc}_{\mathrm{FB}}$ computed on the subset with indicator $s_i=1$.
  \item \textbf{Visible accuracy}: $\mathrm{Acc}_{\mathrm{VIS}}$ computed on the complementary subset with $s_i=0$.
\end{itemize}
Let $\mathcal{I}_{\mathrm{FB}}=\{i: s_i=1\}$ and $\mathcal{I}_{\mathrm{VIS}}=\{i: s_i=0\}$. Then
\begin{equation}
  \mathrm{Acc}_{\mathrm{FB}} = \frac{1}{|\mathcal{I}_{\mathrm{FB}}|} \sum_{i\in\mathcal{I}_{\mathrm{FB}}} \mathbf{1}\{\hat{y}_i=y_i\},
  \qquad
  \mathrm{Acc}_{\mathrm{VIS}} = \frac{1}{|\mathcal{I}_{\mathrm{VIS}}|} \sum_{i\in\mathcal{I}_{\mathrm{VIS}}} \mathbf{1}\{\hat{y}_i=y_i\}.
\end{equation}

These bucketed metrics isolate performance when the agent’s internal belief about the world can be wrong (false-belief) versus when the salient change is visible. Improvements on the false-belief subset are particularly indicative of whether the observer architecture captures latent-state reasoning.

\section{Early Stopping (Experiments)}

In comparison experiments, we employ early stopping on validation accuracy with patience $P$ epochs. If the best validation accuracy does not improve for $P$ consecutive epochs, training halts and the best model metrics are recorded. This reduces overfitting and unnecessary compute, especially for quantum and hybrid models.

Practically, early stopping prevents spending additional time on epochs that are unlikely to yield better generalization. We track and retain the epoch with the highest validation accuracy, reporting those metrics as the model’s best performance.

\section{Putting It Together}

Training iteratively updates $\bm{\theta}$ to minimize expected cross-entropy on action labels. The same loop applies regardless of belief/state encoder type; quantum and hybrid variants simply augment $\bm{\theta}$ with circuit parameters and include the quantum projection/measurement in the forward pass. At convergence, the learned observer parameters encode task-relevant structure across character history, mental context, and state/belief representations, yielding 5-way action logits.

For reproducibility, experiments fix random seeds for Python, NumPy, and PyTorch; device selection (CPU/GPU) is explicit; and data generation hyperparameters (number of agents, episodes, horizon) are logged alongside training settings (epochs, batch size, learning rate). These details ensure that architectural comparisons (classical vs. quantum vs. hybrid) reflect differences in representation rather than incidental variations in the training process.

\end{document}


